{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpftCKnWdyXN",
        "outputId": "f94db9af-5821-4b59-ed84-0fca4ac84dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3798 entries, 0 to 3797\n",
            "Data columns (total 6 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Date    3798 non-null   object \n",
            " 1   Close   3798 non-null   float64\n",
            " 2   High    3798 non-null   float64\n",
            " 3   Low     3798 non-null   float64\n",
            " 4   Open    3798 non-null   float64\n",
            " 5   Volume  3798 non-null   int64  \n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 178.2+ KB\n",
            "None\n",
            "\n",
            "Description of dataset :\n",
            "             Close         High          Low         Open        Volume\n",
            "count  3798.000000  3798.000000  3798.000000  3798.000000  3.798000e+03\n",
            "mean     90.559224    92.608116    88.425626    90.587811  9.709981e+07\n",
            "std     115.245077   117.954878   112.476077   115.345283  7.660983e+07\n",
            "min       1.053333     1.108667     0.998667     1.076000  1.777500e+06\n",
            "25%      12.577667    12.806167    12.289500    12.574500  5.055585e+07\n",
            "50%      19.046000    19.430333    18.700001    19.000334  8.337780e+07\n",
            "75%     194.904999   199.277504   190.559166   195.297497  1.225153e+08\n",
            "max     479.859985   488.540009   457.510010   475.899994  9.140820e+08\n",
            "\n",
            "Missing Values:\n",
            "Date      0\n",
            "Close     0\n",
            "High      0\n",
            "Low       0\n",
            "Open      0\n",
            "Volume    0\n",
            "dtype: int64\n",
            "\n",
            "Numerical Columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
            "\n",
            "After Scaling (StandardScaler):\n",
            "      Close      High       Low      Open    Volume\n",
            "0 -0.772079 -0.771087 -0.775879 -0.774483  2.407249\n",
            "1 -0.772113 -0.768023 -0.772464 -0.770558  2.098005\n",
            "2 -0.773195 -0.770567 -0.774260 -0.771014  0.341805\n",
            "3 -0.774792 -0.772161 -0.775185 -0.772170 -0.261134\n",
            "4 -0.776580 -0.773913 -0.776892 -0.773904  0.077072\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"Tesla_stock_data.csv\")\n",
        "\n",
        "# Step 2 : Explore data\n",
        "# Gives the info of datatypes and more\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Description of dataset\n",
        "print(\"\\nDescription of dataset :\")\n",
        "print(df.describe())\n",
        "\n",
        "# Count of total null values present in each cloumn\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Step 3 : Handle missing data (No missing data in this dataset)\n",
        "# Based on the output of df.isnull().sum(), there are no missing values in this dataset.\n",
        "# Therefore, no missing data handling is needed for this specific dataset.\n",
        "\n",
        "# Step 4 : Handle Categorical Variables (Only 'Date' is object type, but it's not a categorical feature for modeling)\n",
        "# The 'Date' column is of object type, but it represents dates, not categorical features for direct encoding.\n",
        "# If date-related features are needed, they should be extracted from the 'Date' column (e.g., year, month, day).\n",
        "# For now, we won't perform categorical encoding as there are no suitable categorical columns for typical machine learning models in this dataset.\n",
        "\n",
        "# Step 5 : Apply Normalization and Standardization\n",
        "# Identify numerical columns\n",
        "# Exclude 'Date' column as it's not numerical for scaling\n",
        "numerical_cols = df.select_dtypes(include = ['int64', 'float64']).columns.tolist()\n",
        "print(\"\\nNumerical Columns:\", numerical_cols)\n",
        "\n",
        "scaler = StandardScaler();\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "print(\"\\nAfter Scaling (StandardScaler):\")\n",
        "print(df[numerical_cols].head())\n",
        "\n",
        "# Step 6 : Split the dataset\n",
        "# To split the dataset for time series data like stock prices,\n",
        "# a simple random split is usually not appropriate as it disrupts the temporal order.\n",
        "# A common approach is to split based on time.\n",
        "# However, without a defined target variable for a specific task (e.g., predicting 'Close' price),\n",
        "# splitting into features (X) and target (Y) and then into train/test sets\n",
        "# as done in the original code is not applicable for this dataset in its current form.\n",
        "# If you have a specific prediction task in mind, please define the target variable.\n",
        "\n",
        "# The original code was trying to split based on a 'survived' column which does not exist.\n",
        "# Removing the split section as it's not relevant to the current dataset and context.\n",
        "\n",
        "# If you intend to use this data for time series forecasting,\n",
        "# you would typically prepare the data differently,\n",
        "# for example, creating lagged features or using time-aware splitting methods.\n",
        "\n",
        "# For now, the code will perform data exploration, check for missing values,\n",
        "# and apply StandardScaler to the numerical columns."
      ]
    }
  ]
}